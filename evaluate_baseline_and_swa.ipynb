{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a404cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForQuestionAnswering,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from utils import preprocess_and_tokenize, make_predictions, make_predictions_with_swa, post_processing_function\n",
    "from swag_transformers.swag_bert import SwagBertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9a0e78-4a1c-454e-a362-b053e35e2439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111e485ce4be44188f595f6ed7c27d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name) # This supports offsets mapping\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "swag_model = SwagBertForQuestionAnswering.from_base(model, no_cov_mat=False)  # Use SWAG (no_cov_mat=False)\n",
    "model = model.to(device)\n",
    "swag_model = swag_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63967679-20a1-4fd3-8082-4cac7001b6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769452d5941f4637bc44c3bad3cc3683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574f4631c1ce47f3af3fbe7c0dc6e7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c664973ddb4c462d95caa12de9c4587f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SQuAD 2.0 dataset from Hugging Face\n",
    "squad_dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Because test set is hidden, make a new split\n",
    "split = squad_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=False)\n",
    "train_set = split[\"train\"]\n",
    "dev_set = split[\"test\"]\n",
    "test_set = squad_dataset[\"validation\"]\n",
    "\n",
    "# subsets for testing\n",
    "train_set = train_set.select(range(1))  \n",
    "dev_set = dev_set.select(range(10))     \n",
    "test_set = test_set.select(range(1)) \n",
    "\n",
    "# Preprocess the custom splits\n",
    "tokenized_train, tokenized_dev, tokenized_test = preprocess_and_tokenize(train_set, dev_set, test_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7144bf0e-b873-4fd7-9594-983b270a4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3048a2-95e1-4980-9a3f-a349996cd941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e42d2414744f2f87d12f64863efeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  {'exact': 70.0, 'f1': 76.66666666666667, 'total': 10, 'HasAns_exact': 62.5, 'HasAns_f1': 70.83333333333333, 'HasAns_total': 8, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 2, 'best_exact': 70.0, 'best_exact_thresh': 0.0, 'best_f1': 76.66666666666667, 'best_f1_thresh': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No parameters collected yet, you should first run collect_model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9203f3bc47474fa4b949b830df325f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWA:  {'exact': 0.0, 'f1': 0.0, 'total': 10, 'HasAns_exact': 0.0, 'HasAns_f1': 0.0, 'HasAns_total': 8, 'NoAns_exact': 0.0, 'NoAns_f1': 0.0, 'NoAns_total': 2, 'best_exact': 20.0, 'best_exact_thresh': 0.0, 'best_f1': 20.0, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline from checkpoint\n",
    "\n",
    "output_dir = \"./model_1\"\n",
    "checkpoint_dir = f\"{output_dir}/checkpoint-14825\" # epoch 1\n",
    "model = BertForQuestionAnswering.from_pretrained(checkpoint_dir)\n",
    "model = model.to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(output_dir)\n",
    "\n",
    "predictions = make_predictions(model, tokenized_dev)\n",
    "final_predictions = post_processing_function(dev_set, tokenized_dev, predictions)\n",
    "references = [{\"id\": ex['id'], \"answers\": ex['answers']} for ex in dev_set]\n",
    "results = metric.compute(predictions=final_predictions.predictions, references=references)\n",
    "print(\"Baseline: \", results)\n",
    "\n",
    "# Evaluate SWA from checkpoint\n",
    "\n",
    "checkpoint_dir = f\"{output_dir}/checkpoint-swag-epoch-1\" \n",
    "swag_model = SwagBertForQuestionAnswering.from_pretrained(checkpoint_dir)\n",
    "swag_model = swag_model.to(device)\n",
    "\n",
    "predictions = make_predictions_with_swa(swag_model, tokenized_dev)\n",
    "final_predictions = post_processing_function(dev_set, tokenized_dev, predictions)\n",
    "references = [{\"id\": ex['id'], \"answers\": ex['answers']} for ex in dev_set]\n",
    "results = metric.compute(predictions=final_predictions.predictions, references=references)\n",
    "print(\"SWA: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6ea21-6871-4d7d-8645-2c47e59d4c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
